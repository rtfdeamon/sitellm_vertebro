#!/usr/bin/env python
"""run_crawl.py  ‚Äí –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ–±‚Äë–∫—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã

–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–º –ø–æ—Å–ª–µ –¥–µ–ø–ª–æ—è –∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫–∏,
–Ω–∞—á–∏–Ω–∞—è —Å `CRAWL_START_URL` (–∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `--url`). –°—Ç—Ä–∞–Ω–∏—Ü—ã
—Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ MongoDB (–ø–æ¬†—É–º–æ–ª—á–∞–Ω–∏—é mongodb://root:changeme@mongo:27017,
–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è `MONGO_URI`).

–ü—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ —Å—é–¥–∞ –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å:
* —Ä–∞—Å—á—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –æ—Ç–ø—Ä–∞–≤–∫—É –≤ Qdrant;
* –æ—á–µ—Ä–µ–¥–∏ Celery –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫—Ä–∞—É–ª–∏–Ω–≥–∞;
* –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—é –ø–æ¬†MD5/URL‚Äë–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏;
*¬†–ø–∞—Ä—Å–∏–Ω–≥ sitemap.xml –∏ robots.txt.

–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è / CLI‚Äë–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
-----------------------------------
MONGO_URI          ‚Äì —Å—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Mongo ("mongodb://...").
CRAWL_START_URL    ‚Äì —Å—Ç–∞—Ä—Ç–æ–≤–∞—è —Ç–æ—á–∫–∞ –æ–±—Ö–æ–¥–∞.
CRAWL_MAX_PAGES    ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ¬†—É–º–æ–ª—á–∞–Ω–∏—é 500).
CRAWL_MAX_DEPTH    ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (–ø–æ¬†—É–º–æ–ª—á–∞–Ω–∏—é 3).

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ –≤—Ä—É—á–Ω—É—é –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ:
$ python crawler/run_crawl.py --url "https://example.com" --max-pages 1000
"""

from __future__ import annotations

import argparse
import logging
import os
import sys
import time
import urllib.parse as urlparse
from collections import deque
from typing import Iterable, List, Set, Tuple

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, UpdateOne
from redis import Redis
from contextlib import contextmanager

# ----------------------------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ----------------------------- #
DEFAULT_MAX_PAGES: int = int(os.getenv("CRAWL_MAX_PAGES", "500"))
DEFAULT_MAX_DEPTH: int = int(os.getenv("CRAWL_MAX_DEPTH", "3"))
DEFAULT_START_URL: str | None = os.getenv("CRAWL_START_URL")
DEFAULT_MONGO_URI: str = os.getenv(
    "MONGO_URI", "mongodb://root:changeme@mongo:27017"
)
REDIS_PREFIX = os.getenv("STATUS_PREFIX", "crawl:")
r = Redis(host=os.getenv("REDIS_HOST", "redis"), port=int(os.getenv("REDIS_PORT", "6379")), decode_responses=True)

def _incr(key: str, delta: int = 1):
    try:
        r.incrby(REDIS_PREFIX + key, delta)
    except Exception:
        pass

def _set(key: str, value):
    try:
        r.set(REDIS_PREFIX + key, value)
    except Exception:
        pass

def on_crawler_start():
    _set("started_at", time.time())

@contextmanager
def mark_url(url: str):
    _incr("in_progress", 1)
    _incr("queued", -1)

    _set("last_url", url)
    try:
        yield
        _incr("done", 1)
    except Exception:
        _incr("failed", 1)
        raise
    finally:
        _incr("in_progress", -1)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (compatible; SiteLLM-VertebroCrawler/1.0; +https://example.com)"
    )
}
REQUEST_TIMEOUT = float(os.getenv("CRAWL_TIMEOUT", "10"))  # seconds
BATCH_SIZE = 50  # —Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—É—à–∏–º –≤ Mongo –∑–∞ —Ä–∞–∑

# ------------------------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ------------------- #

def fetch(url: str) -> Tuple[str | None, str | None]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (html, content_type)."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        return resp.text, resp.headers.get("content-type", "")
    except Exception as exc:  # noqa: BLE001
        logging.warning("‚ö†Ô∏è  fetch failed %s: %s", url, exc)
        return None, None


def extract_links(html: str, base_url: str) -> List[str]:
    """–í—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç –≤—Å–µ <a href="..."> –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É URL."""
    soup = BeautifulSoup(html, "html.parser")
    links: list[str] = []
    for a in soup.find_all("a", href=True):
        href: str = a["href"]
        abs_url: str = urlparse.urljoin(base_url, href)
        # —É–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è, query‚Äë–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å
        abs_url = abs_url.split("#")[0]
        if abs_url.startswith("http"):
            links.append(abs_url)
    return links


def crawl(
    start_url: str,
    *,
    max_pages: int = DEFAULT_MAX_PAGES,
    max_depth: int = DEFAULT_MAX_DEPTH,
    allowed_domain: str | None = None,
) -> Iterable[Tuple[str, str, str]]:
    """BFS‚Äë–æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞ (non‚Äëblocking, –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞).

    Yields:
        (url, html, content_type)
    """

    visited: Set[str] = set()
    queue: deque[Tuple[str, int]] = deque([(start_url, 0)])
    _incr("queued", 1)

    while queue and len(visited) < max_pages:
        url, depth = queue.popleft()
        if url in visited or depth > max_depth:
            continue
        visited.add(url)

        with mark_url(url):
            html, ctype = fetch(url)
            if html is None:
                continue

            yield url, html, ctype

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ–º–µ–Ω, —á—Ç–æ–±—ã –Ω–µ —É–ø–æ–ª–∑—Ç–∏ –¥–∞–ª–µ–∫–æ
            if allowed_domain and urlparse.urlparse(url).netloc != allowed_domain:
                continue

            for link in extract_links(html, url):
                if link not in visited:
                    queue.append((link, depth + 1))
                    _incr("queued", 1)


# -------------------------- MongoDB utils ---------------------------- #

def get_mongo_collection(
    mongo_uri: str = DEFAULT_MONGO_URI,
    *,
    db_name: str = "crawler",
    collection_name: str = "pages",
):
    client = MongoClient(mongo_uri)
    return client[db_name][collection_name]


def store_batch(col, docs: List[dict]):  # noqa: ANN001
    """Upsert‚Äë–∏—Ç –ø–∞—á–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    requests_: list[UpdateOne] = []
    for doc in docs:
        requests_.append(
            UpdateOne({"url": doc["url"]}, {"$set": doc}, upsert=True)
        )
    if requests_:
        col.bulk_write(requests_)


# ------------------------------ main --------------------------------- #

def main() -> None:  # noqa: D401
    """CLI‚Äë–≤—Ö–æ–¥–Ω–∞—è —Ç–æ—á–∫–∞."""

    parser = argparse.ArgumentParser(description="–ü—Ä–æ—Å—Ç–æ–π —Å–∞–π—Ç‚Äë–∫—Ä–∞—É–ª–µ—Ä ‚Üí MongoDB")
    parser.add_argument("--url", default=DEFAULT_START_URL, help="Start URL to crawl")
    parser.add_argument("--max-pages", type=int, default=DEFAULT_MAX_PAGES)
    parser.add_argument("--max-depth", type=int, default=DEFAULT_MAX_DEPTH)
    parser.add_argument(
        "--domain",
        help="Restrict crawl to this domain (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–º–µ–Ω —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ URL)",
    )
    parser.add_argument(
        "--mongo-uri",
        default=DEFAULT_MONGO_URI,
        help="MongoDB connection string (mongodb://user:pass@host:port)",
    )
    args = parser.parse_args()

    if not args.url:
        sys.exit("‚ùå Need --url or set CRAWL_START_URL")

    domain = args.domain or urlparse.urlparse(args.url).netloc

    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s %(levelname)s ‚ñ∂ %(message)s"
    )
    logging.info("üöÄ Crawler started ‚Üí %s (depth=%s, pages=%s)", args.url, args.max_depth, args.max_pages)

    col = get_mongo_collection(args.mongo_uri)
    buffer: list[dict] = []

    on_crawler_start()

    try:
        for url, html, ctype in crawl(
            args.url,
            max_pages=args.max_pages,
            max_depth=args.max_depth,
            allowed_domain=domain,
        ):
            logging.info("üì• %s", url)
            buffer.append(
                {
                    "url": url,
                    "content_type": ctype,
                    "html": html,
                    "ts": time.time(),
                }
            )
            if len(buffer) >= BATCH_SIZE:
                store_batch(col, buffer)
                buffer.clear()
        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–∏–≤
        if buffer:
            store_batch(col, buffer)

        logging.info("‚úÖ Crawl complete (%s pages).", args.max_pages)
    except KeyboardInterrupt:
        logging.warning("‚èπ Interrupted by user. Flushing buffer‚Ä¶")
        if buffer:
            store_batch(col, buffer)


if __name__ == "__main__":
    main()
