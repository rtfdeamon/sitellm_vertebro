services:
  app:
    build:
      args:
        PIP_EXTRA_INDEX_URL: "https://download.pytorch.org/whl/cu121"
        CMAKE_ARGS: "-DGGML_CUDA=on"
        LLAMA_CPP_PYTHON_BUILD: "cmake"
    environment:
      # Enable CUDA backend in llama-cpp-python build
      CMAKE_ARGS: "-DGGML_CUDA=on"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      # Use CUDA wheels for PyTorch (CUDA 12.1 example)
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cu121"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    gpus: "all"

  celery_worker:
    build:
      args:
        PIP_EXTRA_INDEX_URL: "https://download.pytorch.org/whl/cu121"
        CMAKE_ARGS: "-DGGML_CUDA=on"
        LLAMA_CPP_PYTHON_BUILD: "cmake"
    environment:
      CMAKE_ARGS: "-DGGML_CUDA=on"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cu121"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    gpus: "all"
