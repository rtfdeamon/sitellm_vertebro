# Docker Compose stack to run the API, Redis and Mongo locally
name: sitellm_vertebro
x-health: &health_defaults
  interval: 15s
  timeout: 3s
  retries: 10
  start_period: 20s

services:
  mongo:
    # MongoDB stores conversation history and documents
    image: mongo:8.0
    restart: unless-stopped
    ports:
      - "${HOST_MONGO_PORT:-27027}:27017"
    volumes:
      - "mongo_data:/data/db"
    env_file: .env
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME:-root}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD:-f76DlgezffdHetX}
      MONGO_INITDB_DATABASE: ${MONGO_DATABASE:-smarthelperdb}
    healthcheck:
      <<: *health_defaults
      test: ["CMD", "mongosh", "--quiet", "--eval", "db.adminCommand('ping')"]

  redis:
    # Redis is used for the vector store and Celery broker
    image: bitnami/redis:7.0
    restart: unless-stopped
    env_file: .env
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    ports:
      - "${HOST_REDIS_PORT:-16379}:6379"
    healthcheck:
      <<: *health_defaults
      test: ["CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD} ping"]

  celery_worker:
    # Processes scheduled tasks that update the vector store
    build:
      context: ./
      dockerfile: Dockerfile
      args:
        APT_CACHE_ID: apt-cache
        UV_CACHE_ID: uv-cache
        # Force CPU wheels for PyTorch in CPU mode
        PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu/simple"
    restart: unless-stopped
    env_file: .env
    profiles: ["local-llm"]
    environment:
      REDIS_URL: ${REDIS_URL}
      MONGO_URI: ${MONGO_URI:-mongodb://${MONGO_USERNAME}:${MONGO_PASSWORD}@mongo:27017}
      CRAWL_START_URL: ${CRAWL_START_URL:-https://mmvs.ru}
      CMAKE_ARGS: "-DLLAMA_CUBLAS=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_NATIVE=ON"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu"
    command: ["celery", "-A", "worker", "worker", "--loglevel=INFO"]
    depends_on:
      redis:
        condition: service_healthy
      mongo:
        condition: service_healthy
      celery_beat:
        condition: service_started
    volumes:
      - ./data/hf:/root/.cache/huggingface
    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - pgrep -af "[c]elery.*worker" >/dev/null || exit 1

  celery_beat:
    # Scheduler for periodic vector store updates
    build:
      context: ./
      dockerfile: Dockerfile
      args:
        APT_CACHE_ID: apt-cache
        UV_CACHE_ID: uv-cache
        # Force CPU wheels for PyTorch in CPU mode
        PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu/simple"
    restart: unless-stopped
    env_file: .env
    profiles: ["local-llm"]
    command: ["celery", "-A", "worker", "beat", "--loglevel=INFO"]
    depends_on:
      redis:
        condition: service_healthy
    environment:
      REDIS_URL: ${REDIS_URL}
      CMAKE_ARGS: "-DLLAMA_CUBLAS=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_NATIVE=ON"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu"

    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - pgrep -af "[c]elery.*beat" >/dev/null || exit 1

  app:
    # FastAPI application exposing the LLM endpoints
    build:
      context: ./
      dockerfile: Dockerfile
      args:
        APT_CACHE_ID: apt-cache
        UV_CACHE_ID: uv-cache
        # Force CPU wheels for PyTorch in CPU mode
        PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu/simple"
    restart: unless-stopped
    env_file: .env
    ports:
      - "${HOST_APP_PORT:-18000}:8000"
    environment:
      REDIS_URL: ${REDIS_URL}
      APP_HOST: "0.0.0.0"
      APP_PORT: "8000"
      MONGO_URI: ${MONGO_URI:-mongodb://${MONGO_USERNAME}:${MONGO_PASSWORD}@mongo:27017}
      CRAWL_START_URL: ${CRAWL_START_URL:-https://mmvs.ru}
      # If you run Ollama on the host, this points containers to it
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      PYTHONPATH: /app
    command: [
      "uvicorn", "app:app",
      "--host", "0.0.0.0",
      "--port", "${APP_PORT:-8000}",
      "--workers", "${APP_WORKERS:-1}",
      "--timeout-keep-alive", "30"
    ]
    volumes:
      - ./data/hf:/root/.cache/huggingface
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      interval: 15s
      timeout: 5s
      retries: 30
      start_period: 120s
      test:
        - CMD-SHELL
        - >
          (curl -fsS http://localhost:${APP_PORT:-8000}/healthz ||
           curl -fsS http://localhost:${APP_PORT:-8000}/health ||
           pgrep -af "[u]vicorn.*app:app" >/dev/null) || exit 1
    depends_on:
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    platform: linux/amd64
    # Uncomment the next line if you are on Apple Silicon and face issues
    # platform: linux/arm64
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__LOG_LEVEL: INFO
    volumes:
      - qdrant_data:/qdrant/storage
    # Note: healthcheck removed to avoid false negatives on minimal images.


  telegram-bot:
    build:
      context: .
      dockerfile: docker/Dockerfile.tg_bot
    env_file: .env
    environment:
      PYTHONPATH: /app
    restart: unless-stopped
    depends_on:
      app:
        condition: service_started
    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - >
          pgrep -af "python.*tg_bot\.run" >/dev/null || exit 1

volumes:
  mongo_data:
  qdrant_data:
