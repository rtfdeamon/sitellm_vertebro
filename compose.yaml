# Docker Compose stack to run the API, Redis and Mongo locally
name: sitellm_vertebro
x-backend-build: &backend-build
  context: ./
  dockerfile: Dockerfile
  args:
    APT_CACHE_ID: apt-cache
    UV_CACHE_ID: uv-cache
    # Force CPU wheels for PyTorch in CPU mode
    PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu/simple"
x-health: &health_defaults
  interval: 15s
  timeout: 3s
  retries: 10
  start_period: 20s

services:
  mongo:
    # MongoDB stores conversation history and documents
    image: mongo:8.0
    restart: unless-stopped
    ports:
      - "${HOST_MONGO_PORT:-27027}:27017"
    volumes:
      - "mongo_data:/data/db"
    env_file: .env
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME:-root}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD:-f76DlgezffdHetX}
      MONGO_INITDB_DATABASE: ${MONGO_DATABASE:-smarthelperdb}
    healthcheck:
      <<: *health_defaults
      test: ["CMD", "mongosh", "--quiet", "--eval", "db.adminCommand('ping')"]

  redis:
    # Redis acts as Celery broker and cache for application responses
    image: ${REDIS_IMAGE:-bitnami/redis:latest}
    restart: unless-stopped
    env_file: .env
    command:
      - redis-server
      - "--appendonly"
      - "no"
      - "--requirepass"
      - "${REDIS_PASSWORD}"
    ports:
      - "${HOST_REDIS_PORT:-16379}:6379"
    healthcheck:
      <<: *health_defaults
      test: ["CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD} ping"]

  celery_worker:
    # Processes scheduled tasks that update the vector store
    image: ${BACKEND_IMAGE:-sitellm/backend}:${BACKEND_VERSION:-1}
    build: *backend-build
    restart: unless-stopped
    env_file: .env
    environment:
      REDIS_URL: ${REDIS_URL}
      MONGO_URI: ${MONGO_URI:-mongodb://${MONGO_USERNAME}:${MONGO_PASSWORD}@mongo:27017}
      CRAWL_START_URL: ${CRAWL_START_URL:-https://mmvs.ru}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      CMAKE_ARGS: "-DLLAMA_CUBLAS=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_NATIVE=ON"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu"
    command: ["celery", "-A", "worker", "worker", "--loglevel=INFO"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
      mongo:
        condition: service_healthy
      celery_beat:
        condition: service_started
      ollama:
        condition: service_started
    volumes:
      - ./data/hf:/root/.cache/huggingface
    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - pgrep -af "[c]elery.*worker" >/dev/null || exit 1

  celery_beat:
    # Scheduler for periodic vector store updates
    image: ${BACKEND_IMAGE:-sitellm/backend}:${BACKEND_VERSION:-1}
    build: *backend-build
    restart: unless-stopped
    env_file: .env
    command: ["celery", "-A", "worker", "beat", "--loglevel=INFO"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
    environment:
      REDIS_URL: ${REDIS_URL}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      CMAKE_ARGS: "-DLLAMA_CUBLAS=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_NATIVE=ON"
      LLAMA_CPP_PYTHON_BUILD: "cmake"
      PIP_INDEX_URL: "https://download.pytorch.org/whl/cpu"

    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - pgrep -af "[c]elery.*beat" >/dev/null || exit 1

  ollama:
    # Local Ollama runtime hosting GGUF models
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "${HOST_OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      interval: 30s
      timeout: 5s
      retries: 10
      test:
        - CMD
        - ollama
        - ps

  app:
    # FastAPI application exposing the LLM endpoints
    image: ${BACKEND_IMAGE:-sitellm/backend}:${BACKEND_VERSION:-1}
    build: *backend-build
    restart: unless-stopped
    env_file: .env
    ports:
      - "${HOST_APP_PORT:-18000}:8000"
    environment:
      REDIS_URL: ${REDIS_URL}
      APP_HOST: "0.0.0.0"
      APP_PORT: "8000"
      MONGO_URI: ${MONGO_URI:-mongodb://${MONGO_USERNAME}:${MONGO_PASSWORD}@mongo:27017}
      CRAWL_START_URL: ${CRAWL_START_URL:-https://mmvs.ru}
      DOMAIN: ${DOMAIN:-}
      # Ollama runtime inside compose (fall back to host when overridden)
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      PYTHONPATH: /app
      APP_SSL_CERT: ${APP_SSL_CERT:-/certs/server.crt}
      APP_SSL_KEY: ${APP_SSL_KEY:-/certs/server.key}
      APP_ENABLE_TLS: ${APP_ENABLE_TLS:-0}
    volumes:
      - ./data/hf:/root/.cache/huggingface
      - ./certs:/certs:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      interval: 15s
      timeout: 5s
      retries: 30
      start_period: 120s
      test:
        - CMD-SHELL
        - >
          (curl -fsS http://localhost:${APP_PORT:-8000}/healthz ||
           curl -fsS http://localhost:${APP_PORT:-8000}/health ||
           pgrep -af "[u]vicorn.*app:app" >/dev/null) || exit 1
    depends_on:
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
      qdrant:
        condition: service_started

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    platform: ${QDRANT_PLATFORM:-linux/amd64}
    # Uncomment the next line if you are on Apple Silicon and face issues
    # platform: linux/arm64
    env_file: .env
    environment:
      QDRANT__SERVICE__HTTP_PORT: ${QDRANT_HTTP_PORT:-6333}
      QDRANT__SERVICE__GRPC_PORT: ${QDRANT_GRPC_PORT:-6334}
      QDRANT__LOG_LEVEL: ${QDRANT_LOG_LEVEL:-INFO}
    ports:
      - "${HOST_QDRANT_HTTP_PORT:-26333}:${QDRANT_HTTP_PORT:-6333}"
      - "${HOST_QDRANT_GRPC_PORT:-26334}:${QDRANT_GRPC_PORT:-6334}"
    volumes:
      - qdrant_data:/qdrant/storage
    # Note: healthcheck removed to avoid false negatives on minimal images.


  telegram-bot:
    image: ${TELEGRAM_IMAGE:-sitellm/telegram}:${TELEGRAM_VERSION:-1}
    build:
      context: .
      dockerfile: docker/Dockerfile.tg_bot
    env_file: .env
    environment:
      PYTHONPATH: /app
    restart: unless-stopped
    depends_on:
      app:
        condition: service_started
    volumes:
      - ./certs:/certs:ro
    healthcheck:
      <<: *health_defaults
      test:
        - CMD-SHELL
        - >
          pgrep -af "python.*tg_bot\.run" >/dev/null || exit 1

volumes:
  mongo_data:
  qdrant_data:
  ollama_data:
